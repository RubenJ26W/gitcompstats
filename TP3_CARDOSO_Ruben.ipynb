{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ruben CARDOSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTATIONAL STATISTICS: HASTING METROPOLIS AND GIBBS SAMPLER.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "### Q1 :\n",
    "\n",
    "First we know that we have : \n",
    "$$\n",
    "q(y,z,\\theta)=p(\\theta)\\,p(z\\mid\\theta)\\,p(y\\mid z,\\theta).\n",
    "$$\n",
    "With \n",
    "$$\n",
    "p(\\theta)\n",
    "= p(\\bar t_0)\\,p(\\bar v_0)\\,p(\\sigma_\\xi^2)\\,p(\\sigma_\\tau^2)\\,p(\\sigma^2).\n",
    "$$\n",
    "\n",
    "and : \n",
    "\n",
    "$$\n",
    "p(z\\mid\\theta)\n",
    "= p(t_0\\mid\\bar t_0)\\,p(v_0\\mid\\bar v_0)\\,\n",
    "\\prod_{i=1}^N p(\\xi_i\\mid\\sigma_\\xi^2)\\,p(\\tau_i\\mid\\sigma_\\tau^2).\n",
    "$$\n",
    "\n",
    "Since :\n",
    "$$\n",
    "y_{i,j}\\mid z_{\\mathrm{pop}},z_i,\\theta \n",
    "\\sim \n",
    "\\mathcal N\\!\\left(p_0+v_0\\alpha_i(t_{i,j}-t_0-\\tau_i),\\,\\sigma^2\\right), \\quad \\forall 1\\leq i \\leq N ; \\quad \\forall 1\\leq j \\leq k_i \n",
    "$$\n",
    "We then have that : \n",
    "$$\n",
    "p(y\\mid z,\\theta)\n",
    "= \\prod_{i=1}^N\\prod_{j=1}^{k_i}\n",
    "\\mathcal N\\!\\left(\n",
    "y_{i,j}\\,;\\,\n",
    "\\mathbb{E}\\big[y_{i,j}\\mid z_{\\mathrm{pop}},z_i,\\theta\\big],\\,\n",
    "\\mathbf{Var}\\big[y_{i,j}\\mid z_{\\mathrm{pop}},z_i,\\theta\\big]\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Such that $\\mathbb{E}\\big[y_{i,j}\\mid z_{\\mathrm{pop}},z_i,\\theta\\big]=p_0 + v_0 \\alpha_i (t_{i,j} - t_0 - \\tau_i).$ and $\\mathbf{Var}\\big[y_{i,j}\\mid z_{\\mathrm{pop}},z_i,\\theta\\big]=\\sigma^2$\n",
    "\n",
    "\n",
    "\n",
    "Therefore : \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log q(y,z,\\theta)\n",
    "&=\n",
    "\\log p(\\bar t_0)\n",
    "+\\log p(\\bar v_0)\n",
    "+\\log p(\\sigma_\\xi^2)\n",
    "+\\log p(\\sigma_\\tau^2)\n",
    "+\\log p(\\sigma^2)\n",
    "\\\\\n",
    "&\\quad\n",
    "+\\log p(t_0\\mid\\bar t_0)\n",
    "+\\log p(v_0\\mid\\bar v_0)\n",
    "+\\sum_{i=1}^N\n",
    "\\left[\n",
    "\\log p(\\xi_i\\mid\\sigma_\\xi^2)\n",
    "+\\log p(\\tau_i\\mid\\sigma_\\tau^2)\n",
    "\\right]\n",
    "\\\\\n",
    "&\\quad\n",
    "+\\sum_{i=1}^N\\sum_{j=1}^{k_i}\n",
    "\\log p\\big(y_{i,j}\\mid t_0,v_0,\\xi_i,\\tau_i,\\sigma^2\\big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Expansion of the terms\n",
    "\n",
    "Data:\n",
    "$$\n",
    "\\log p(y\\mid z,\\theta)\n",
    "=\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^N\\sum_{j=1}^{k_i}\n",
    "\\left(y_{i,j}-p_0-v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i)\\right)^2\n",
    "-\\frac{1}{2}\\Big(\\sum_{i=1}^N k_i\\Big)\\log\\sigma^2.\n",
    "$$\n",
    "\n",
    "Random effects:\n",
    "$$\n",
    "\\sum_{i=1}^N \\log p(\\xi_i\\mid\\sigma_\\xi^2)\n",
    "=\n",
    "-\\frac{1}{2\\sigma_\\xi^2}\\sum_{i=1}^N \\xi_i^2\n",
    "-\\frac{N}{2}\\log\\sigma_\\xi^2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\log p(\\tau_i\\mid\\sigma_\\tau^2)\n",
    "=\n",
    "-\\frac{1}{2\\sigma_\\tau^2}\\sum_{i=1}^N \\tau_i^2\n",
    "-\\frac{N}{2}\\log\\sigma_\\tau^2.\n",
    "$$\n",
    "\n",
    "Population effects:\n",
    "$$\n",
    "\\log p(t_0\\mid\\bar t_0)\n",
    "= -\\frac{(t_0-\\bar t_0)^2}{2s_{t_0}^2},\n",
    "\\qquad\n",
    "\\log p(v_0\\mid\\bar v_0)\n",
    "= -\\frac{(v_0-\\bar v_0)^2}{2s_{v_0}^2}.\n",
    "$$\n",
    "\n",
    "Hyper-parameter priors:\n",
    "$$\n",
    "\\log p(\\bar t_0)\n",
    "= -\\frac{(\\bar t_0-\\bar{\\bar t}_0)^2}{2s_{t_0}^2},\n",
    "\\qquad\n",
    "\\log p(\\bar v_0)\n",
    "= -\\frac{(\\bar v_0-\\bar{\\bar v}_0)^2}{2s_{v_0}^2}.\n",
    "$$\n",
    "\n",
    "We use the exact scalar inverse–Wishart prior given in the handout:\n",
    "$$\n",
    "f_{W^{-1}}(\\sigma^2)\n",
    "= \\frac{1}{\\Gamma\\!\\left(\\frac{m}{2}\\right)}\\,\n",
    "\\frac{1}{\\sigma^2}\\,\n",
    "\\left(\\frac{v}{\\sigma\\sqrt{2}}\\right)^m\n",
    "\\exp\\!\\left(-\\frac{v^2}{2\\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "Taking logs:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(\\sigma^2)\n",
    "&= -\\log\\Gamma\\!\\left(\\frac{m}{2}\\right)\n",
    "   - \\log \\sigma^2\n",
    "   + m \\log\\!\\left(\\frac{v}{\\sigma\\sqrt{2}}\\right)\n",
    "   - \\frac{v^2}{2\\sigma^2} \\\\[0.2cm]\n",
    "&= -\\log\\Gamma\\!\\left(\\frac{m}{2}\\right)\n",
    "   - \\log \\sigma^2\n",
    "   + m\\big(\\log v - \\tfrac12\\log 2 - \\log\\sigma\\big)\n",
    "   - \\frac{v^2}{2\\sigma^2} \\\\[0.2cm]\n",
    "&= \\text{const}\n",
    "   - \\log \\sigma^2\n",
    "   - m \\log\\sigma\n",
    "   - \\frac{v^2}{2\\sigma^2}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $\\log\\sigma = \\tfrac12 \\log\\sigma^2$, we finally obtain\n",
    "$$\n",
    "\\log p(\\sigma^2)\n",
    "= -\\left(1+\\frac{m}{2}\\right)\\log\\sigma^2\n",
    "  -\\frac{v^2}{2\\sigma^2}\n",
    "  + \\text{const},\n",
    "$$\n",
    "and the same form holds for $\\sigma_\\xi^2$ and $\\sigma_\\tau^2$ with their\n",
    "respective $(v,m)$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to show that the complete model\n",
    "$$\n",
    "q(y,z,\\theta)=p(\\theta)\\,p(z\\mid\\theta)\\,p(y\\mid z,\\theta)\n",
    "$$\n",
    "belongs to a (curved) exponential family.\n",
    "\n",
    "Up to constants, the part of the complete log-density that depends on both \\((y,z)\\) and \\(\\theta\\) can be written as\n",
    "$$\n",
    "\\log q(y,z,\\theta)\n",
    "=\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^N\\sum_{j=1}^{k_i}\n",
    "\\bigl(y_{i,j}-p_0-v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i)\\bigr)^2\n",
    "-\\frac{1}{2\\sigma_\\xi^2}\\sum_{i=1}^N \\xi_i^2\n",
    "-\\frac{1}{2\\sigma_\\tau^2}\\sum_{i=1}^N \\tau_i^2\n",
    "-\\frac{K}{2}\\log\\sigma^2\n",
    "+\\text{(terms depending only on }\\theta),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "K = \\sum_{i=1}^N k_i\n",
    "$$\n",
    "is the total number of observations.\n",
    "\n",
    "We introduce the following complete sufficient statistics:\n",
    "\n",
    "- Data misfit:\n",
    "  $$\n",
    "  S_1(y,z)\n",
    "  =\n",
    "  \\sum_{i=1}^N\\sum_{j=1}^{k_i}\n",
    "  \\bigl(y_{i,j}-p_0-v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i)\\bigr)^2.\n",
    "  $$\n",
    "- Random effects for the acceleration:\n",
    "  $$\n",
    "  S_2(z) = \\sum_{i=1}^N \\xi_i^2.\n",
    "  $$\n",
    "- Random effects for the time shift:\n",
    "  $$\n",
    "  S_3(z) = \\sum_{i=1}^N \\tau_i^2.\n",
    "  $$\n",
    "- Total number of observations (for the $\\log\\sigma^2$ term):\n",
    "  $$\n",
    "  S_4(y) = K = \\sum_{i=1}^N k_i.\n",
    "  $$\n",
    "\n",
    "We collect them into\n",
    "$$\n",
    "S(y,z) = \\bigl(S_1,S_2,S_3,S_4\\bigr)(y,z)\\in\\mathbb{R}^4.\n",
    "$$\n",
    "These are linear or quadratic functions of \\((y,z)\\) and do not depend on \\(\\theta\\).\n",
    "\n",
    "With this notation, the part of the log-density involving \\((y,z)\\) can be rewritten as\n",
    "$$\n",
    "\\log q(y,z,\\theta)\n",
    "=\n",
    "\\langle \\phi(\\theta),\\, S(y,z)\\rangle\n",
    "- \\psi(\\theta)\n",
    "+ \\text{const},\n",
    "$$\n",
    "where the natural parameter is\n",
    "$$\n",
    "\\phi(\\theta)\n",
    "=\n",
    "\\left(\n",
    "-\\frac{1}{2\\sigma^2},\n",
    "-\\frac{1}{2\\sigma_\\xi^2},\n",
    "-\\frac{1}{2\\sigma_\\tau^2},\n",
    "-\\frac{1}{2}\\log\\sigma^2\n",
    "\\right)\\in\\mathbb{R}^4,\n",
    "$$\n",
    "and $\\psi(\\theta)$ gathers all remaining terms that depend only on $\\theta$\n",
    "(inverse–Wishart penalties on $\\sigma^2,\\sigma_\\xi^2,\\sigma_\\tau^2$, Gaussian priors on $t_0,\\bar t_0,v_0,\\bar v_0$, etc.).\n",
    "\n",
    "Thus the model is an exponential family with sufficient statistics $S(y,z)$ and natural parameter vector $\\phi(\\theta)$.\n",
    "\n",
    "In a full exponential family, the natural parameter $\\eta$ is free in an open subset of $\\mathbb{R}^d$. Here the natural parameter is\n",
    "$$\n",
    "\\eta(\\theta) = \\phi(\\theta),\n",
    "$$\n",
    "but its components are not free. For instance,\n",
    "$$\n",
    "\\phi_1(\\theta) = -\\frac{1}{2\\sigma^2}\n",
    "\\quad\\text{and}\\quad\n",
    "\\phi_4(\\theta) = -\\frac{1}{2}\\log\\sigma^2\n",
    "$$\n",
    "satisfy the smooth relation\n",
    "$$\n",
    "\\phi_4(\\theta)\n",
    "=\n",
    "\\frac{1}{2}\\log\\bigl(-2\\phi_1(\\theta)\\bigr).\n",
    "$$\n",
    "Hence the image $\\{\\phi(\\theta):\\theta\\in\\Theta\\}$ is a lower-dimensional smooth manifold inside $\\mathbb{R}^4$.\n",
    "\n",
    "Therefore, the complete model $q(y,z,\\theta)$ belongs to a **curved exponential family** (also called a curved exponential linear model, since $S(y,z)$ is linear/quadratic in $(y,z)$ and all the curvature comes from the nonlinear map $\\theta\\mapsto\\phi(\\theta)$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 : \n",
    "\n",
    "Let us take \n",
    "- parameters :\n",
    "  - $p_0 = 0$\n",
    "  - number of individuals: $N = 50$\n",
    "  - number of measurements per individual: $k_i = k = 8$\n",
    "  - observation times: $t_{i,j} = j$ or any grid on $[0,10]$\n",
    "\n",
    "- Hyper-parameters:\n",
    "  - $\\bar t_0 = 0,\\quad s_{t_0} = 1$\n",
    "  - $\\bar v_0 = 1,\\quad s_{v_0} = 0.5$\n",
    "  - $\\sigma_\\xi^2 = 0.2$\n",
    "  - $\\sigma_\\tau^2 = 1.0$\n",
    "  - $\\sigma^2 = 0.5^2$\n",
    "\n",
    "We will **fix** these values (no need to sample the priors) and then generate one synthetic dataset.\n",
    "\n",
    "\n",
    "#### Step 1 – Sample population effects\n",
    "\n",
    "- Sample  \n",
    "  $t_0 \\sim \\mathcal N(\\bar t_0,\\, s_{t_0}^2)$\n",
    "- Sample  \n",
    "  $v_0 \\sim \\mathcal N(\\bar v_0,\\, s_{v_0}^2)$\n",
    "\n",
    "\n",
    "\n",
    "#### Step 2 – For each individual $i = 1,\\dots,N$\n",
    "\n",
    "##### 1. Sample random effects:\n",
    "\n",
    "- $\\xi_i \\sim \\mathcal N(0,\\sigma_\\xi^2)$\n",
    "- $\\alpha_i = \\exp(\\xi_i)$\n",
    "- $\\tau_i \\sim \\mathcal N(0,\\sigma_\\tau^2)$\n",
    "\n",
    "##### 2. For each measurement time $t_{i,j}$, $j=1,\\dots,k$:\n",
    "\n",
    "- Compute the noise-free trajectory:\n",
    "  $\n",
    "  d_i(t_{i,j})\n",
    "  = p_0 + v_0 \\,\\alpha_i \\,(t_{i,j} - t_0 - \\tau_i)\n",
    "  $\n",
    "\n",
    "- Sample observation noise: $\\varepsilon_{i,j} \\sim \\mathcal N(0,\\sigma^2)$\n",
    "\n",
    "- Define the observed measurement:$y_{i,j} = d_i(t_{i,j}) + \\varepsilon_{i,j}.$\n",
    "\n",
    "This generates a full synthetic dataset  $\\{(t_{i,j},\\, y_{i,j}) : i=1,\\dots,N,\\ j=1,\\dots,k\\}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python code to generate one dataset\n",
    "\n",
    "# ---------- parameters ----------\n",
    "N = 50          # number of individuals\n",
    "k = 8           # number of measurements per individual\n",
    "p0 = 0.0\n",
    "\n",
    "tbar0 = 0.0\n",
    "s_t0 = 1.0\n",
    "\n",
    "vbar0 = 1.0\n",
    "s_v0 = 0.5\n",
    "\n",
    "sigma_xi2 = 0.2      # variance of xi\n",
    "sigma_tau2 = 1.0     # variance of tau\n",
    "sigma2 = 0.5**2      # measurement noise variance\n",
    "\n",
    "# measurement times: same grid for everyone here\n",
    "t_grid = np.linspace(0.0, 10.0, k)\n",
    "\n",
    "# ---------- Step 1: population effects ----------\n",
    "t0 = np.random.normal(tbar0, s_t0)\n",
    "v0 = np.random.normal(vbar0, s_v0)\n",
    "\n",
    "# ---------- Containers ----------\n",
    "t = np.zeros((N, k))\n",
    "y = np.zeros((N, k))\n",
    "\n",
    "# ---------- Step 2: individual random effects and data ----------\n",
    "for i in range(N):\n",
    "    xi_i = np.random.normal(0.0, np.sqrt(sigma_xi2))\n",
    "    alpha_i = np.exp(xi_i)\n",
    "    tau_i = np.random.normal(0.0, np.sqrt(sigma_tau2))\n",
    "\n",
    "    for j in range(k):\n",
    "        t_ij = t_grid[j]\n",
    "        mean_ij = p0 + v0 * alpha_i * (t_ij - t0 - tau_i)\n",
    "        eps_ij = np.random.normal(0.0, np.sqrt(sigma2))\n",
    "\n",
    "        t[i, j] = t_ij\n",
    "        y[i, j] = mean_ij + eps_ij\n",
    "\n",
    "# y[i, j] are the synthetic observations,\n",
    "# t[i, j] the corresponding times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: \n",
    "\n",
    "We want to sample the latent variable  \n",
    "$$\n",
    "z = (t_0, v_0, (\\xi_1,\\dots,\\xi_N),(\\tau_1,\\dots,\\tau_N)) \\in \\mathbb{R}^{2N+2}\n",
    "$$\n",
    "from the posterior distribution  \n",
    "$$\n",
    "\\pi(z)=p(z\\mid y,\\theta)\n",
    "\\propto p(y\\mid z,\\theta)\\,p(z\\mid \\theta).\n",
    "$$\n",
    "\n",
    "As given in the assignment statement, we use a Gaussian random-walk proposal  \n",
    "$$\n",
    "z^\\star \\sim \\mathcal{N}(z^{(k)},\\,\\Sigma_q)\n",
    "$$\n",
    "with diagonal covariance  \n",
    "$$\n",
    "\\Sigma_q = \\mathrm{diag}(\\sigma_{t_0}^2,\\sigma_{v_0}^2,\\sigma_\\xi^2,\\sigma_\\tau^2).\n",
    "$$\n",
    "Since the proposal is symmetric, the Hastings ratio reduces to a likelihood ratio.\n",
    "\n",
    "The log-likelihood contribution is  \n",
    "$$\n",
    "\\log p(y\\mid z,\\theta)\n",
    "=\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^N\n",
    "\\sum_{j=1}^{k_i}\n",
    "\\left(\n",
    "y_{i,j} -\n",
    "p_0 - v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i)\n",
    "\\right)^2\n",
    "+ C.\n",
    "$$\n",
    "\n",
    "The prior contribution is  \n",
    "$$\n",
    "\\log p(z\\mid \\theta)\n",
    "=\n",
    "-\\frac{1}{2\\sigma_\\xi^2}\\sum_{i=1}^N \\xi_i^2\n",
    "-\\frac{1}{2\\sigma_\\tau^2}\\sum_{i=1}^N \\tau_i^2\n",
    "-\\frac{(t_0-\\bar t_0)^2}{2 s_{t_0}^2}\n",
    "-\\frac{(v_0-\\bar v_0)^2}{2 s_{v_0}^2}\n",
    "+ C.\n",
    "$$\n",
    "\n",
    "Given the current state $z^{(k)}$ and a proposal $z^\\star$, the acceptance probability is  \n",
    "$$\n",
    "\\alpha\n",
    "=\n",
    "\\min\\Bigl(\n",
    "1,\\;\n",
    "\\exp\\bigl(\\log\\pi(z^\\star)-\\log\\pi(z^{(k)})\\bigr)\n",
    "\\Bigr).\n",
    "$$\n",
    "\n",
    "The Metropolis–Hastings update proceeds as follows.  \n",
    "Start from an initial value $z^{(0)}$ (for instance sampled from the priors).  \n",
    "At iteration $k$:\n",
    "\n",
    "1. Draw a proposal  \n",
    "   $$\n",
    "   z^\\star \\sim \\mathcal{N}(z^{(k)},\\Sigma_q).\n",
    "   $$\n",
    "\n",
    "2. Compute $\\log\\pi(z^\\star)$ and $\\log\\pi(z^{(k)})$.\n",
    "\n",
    "3. Compute the acceptance probability  \n",
    "   $$\n",
    "   \\alpha = \\min\\Bigl(1,\\exp(\\log\\pi(z^\\star)-\\log\\pi(z^{(k)}))\\Bigr).\n",
    "   $$\n",
    "\n",
    "4. Draw $u \\sim \\mathcal{U}(0,1)$.\n",
    "\n",
    "5. Accept or reject:\n",
    "   - if $u \\le \\alpha$, set $z^{(k+1)} = z^\\star$  \n",
    "   - otherwise, set $z^{(k+1)} = z^{(k)}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4:\n",
    "\n",
    "In the SAEM framework, we replace $(S_1,S_2,S_3)$ by their stochastic approximation\n",
    "$$\n",
    "S_k = \\bigl(S_{1,k},S_{2,k},S_{3,k},S_{4,k}\\bigr)\n",
    "\\approx \\mathbb E\\bigl[ S(y,z)\\mid y,\\theta^{(k)}\\bigr].\n",
    "$$\n",
    "\n",
    "The $Q$-function at iteration $k$ is, up to a constant,\n",
    "$$\n",
    "Q_k(\\theta)\n",
    "=\n",
    "-\\frac{1}{2\\sigma^2} S_{1,k}\n",
    "-\\frac{1}{2\\sigma_\\xi^2} S_{2,k}\n",
    "-\\frac{1}{2\\sigma_\\tau^2} S_{3,k}\n",
    "-\\frac{K}{2}\\log\\sigma^2\n",
    "-\\Bigl(1+\\frac{m}{2}\\Bigr)\\log\\sigma^2\n",
    "-\\frac{v^2}{2\\sigma^2}\n",
    "$$\n",
    "$$\n",
    "\\hspace{2cm}\n",
    "-\\frac{N}{2}\\log\\sigma_\\xi^2\n",
    "-\\Bigl(1+\\frac{m_\\xi}{2}\\Bigr)\\log\\sigma_\\xi^2\n",
    "-\\frac{v_\\xi^2}{2\\sigma_\\xi^2}\n",
    "-\\frac{N}{2}\\log\\sigma_\\tau^2\n",
    "-\\Bigl(1+\\frac{m_\\tau}{2}\\Bigr)\\log\\sigma_\\tau^2\n",
    "-\\frac{v_\\tau^2}{2\\sigma_\\tau^2}\n",
    "+\\text{terms in }(t_0,\\bar t_0,v_0,\\bar v_0).\n",
    "$$\n",
    "\n",
    "Maximizing $Q_k(\\theta)$ w.r.t. each variance gives closed-form MAP updates.\n",
    "\n",
    "For $\\sigma^2$ we maximise\n",
    "$$\n",
    "\\ell(\\sigma^2)\n",
    "=\n",
    "-\\frac{S_{1,k}+v^2}{2\\sigma^2}\n",
    "-\\frac{K+2+m}{2}\\log\\sigma^2,\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial\\sigma^2}\n",
    "=\n",
    "\\frac{S_{1,k}+v^2}{2\\sigma^4}\n",
    "-\\frac{K+2+m}{2\\sigma^2}\n",
    "=0\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\sigma^{2,(k+1)}\n",
    "=\n",
    "\\frac{S_{1,k}+v^2}{K+m+2}.\n",
    "$$\n",
    "\n",
    "For $\\sigma_\\xi^2$ we obtain\n",
    "$$\n",
    "\\ell(\\sigma_\\xi^2)\n",
    "=\n",
    "-\\frac{S_{2,k}+v_\\xi^2}{2\\sigma_\\xi^2}\n",
    "-\\frac{N+2+m_\\xi}{2}\\log\\sigma_\\xi^2,\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\sigma_{\\xi}^{2,(k+1)}\n",
    "=\n",
    "\\frac{S_{2,k}+v_\\xi^2}{N+m_\\xi+2}.\n",
    "$$\n",
    "\n",
    "For $\\sigma_\\tau^2$ similarly,\n",
    "$$\n",
    "\\ell(\\sigma_\\tau^2)\n",
    "=\n",
    "-\\frac{S_{3,k}+v_\\tau^2}{2\\sigma_\\tau^2}\n",
    "-\\frac{N+2+m_\\tau}{2}\\log\\sigma_\\tau^2,\n",
    "$$\n",
    "hence\n",
    "$$\n",
    "\\sigma_{\\tau}^{2,(k+1)}\n",
    "=\n",
    "\\frac{S_{3,k}+v_\\tau^2}{N+m_\\tau+2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now detail the derivation of the closed-form MAP update for the hyper–mean $\\bar t_0$ (and similarly for $\\bar v_0$).\n",
    "\n",
    "We use the hierarchical priors\n",
    "$$\n",
    "t_0 \\mid \\bar t_0 \\sim \\mathcal N(\\bar t_0,\\sigma_{t_0}^2),\n",
    "\\qquad\n",
    "\\bar t_0 \\sim \\mathcal N(\\tilde t_0,s_{\\bar t_0}^2),\n",
    "$$\n",
    "with $\\sigma_{t_0}^2$ and $s_{\\bar t_0}^2$ fixed.\n",
    "\n",
    "In the SAEM framework, the part of the $Q$–function that depends on $\\bar t_0$ is\n",
    "$$\n",
    "-\\frac{1}{2\\sigma_{t_0}^2}\\,\n",
    "\\mathbb E\\!\\big[(t_0-\\bar t_0)^2 \\mid y,\\theta^{(k)}\\big]\n",
    "-\\frac{1}{2s_{\\bar t_0}^2}\\,(\\bar t_0-\\tilde t_0)^2\n",
    "+\\text{const}.\n",
    "$$\n",
    "Define the posterior moments\n",
    "$$\n",
    "m_{t,k} := \\mathbb E[t_0 \\mid y,\\theta^{(k)}],\n",
    "\\qquad\n",
    "v_{t,k} := \\mathbb E[t_0^2 \\mid y,\\theta^{(k)}].\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\mathbb E[(t_0-\\bar t_0)^2 \\mid y,\\theta^{(k)}]\n",
    "=\n",
    "\\mathbb E[t_0^2]-2\\bar t_0\\mathbb E[t_0]+\\bar t_0^2\n",
    "=\n",
    "v_{t,k}-2\\bar t_0 m_{t,k}+\\bar t_0^2.\n",
    "$$\n",
    "Up to an additive constant (independent of $\\bar t_0$), we can write\n",
    "$$\n",
    "-\\frac{1}{2\\sigma_{t_0}^2}\\big(v_{t,k}-2\\bar t_0 m_{t,k}+\\bar t_0^2\\big)\n",
    "-\\frac{1}{2s_{\\bar t_0}^2}\\big(\\bar t_0^2-2\\bar t_0 \\tilde t_0\\big)\n",
    "+\\text{const}.\n",
    "$$\n",
    "Discarding the terms that do not contain $\\bar t_0$, this is equivalent to minimising\n",
    "$$\n",
    "\\bar t_0 \\mapsto\n",
    "\\frac{1}{2\\sigma_{t_0}^2}\\big(\\bar t_0^2-2\\bar t_0 m_{t,k}\\big)\n",
    "+\n",
    "\\frac{1}{2s_{\\bar t_0}^2}\\big(\\bar t_0^2-2\\bar t_0 \\tilde t_0\\big).\n",
    "$$\n",
    "Differentiate with respect to $\\bar t_0$:\n",
    "$$\n",
    "\\frac{1}{\\sigma_{t_0}^2}(\\bar t_0-m_{t,k})\n",
    "+\n",
    "\\frac{1}{s_{\\bar t_0}^2}(\\bar t_0-\\tilde t_0).\n",
    "$$\n",
    "Set this derivative to zero:\n",
    "$$\n",
    "\\frac{1}{\\sigma_{t_0}^2}(\\bar t_0-m_{t,k})\n",
    "+\n",
    "\\frac{1}{s_{\\bar t_0}^2}(\\bar t_0-\\tilde t_0)\n",
    "=0.\n",
    "$$\n",
    "Rearranging,\n",
    "$$\n",
    "\\bar t_0\\left(\\frac{1}{\\sigma_{t_0}^2}+\\frac{1}{s_{\\bar t_0}^2}\\right)\n",
    "=\n",
    "\\frac{m_{t,k}}{\\sigma_{t_0}^2}\n",
    "+\n",
    "\\frac{\\tilde t_0}{s_{\\bar t_0}^2},\n",
    "$$\n",
    "so the closed-form MAP update is\n",
    "$$\n",
    "\\bar t_0^{(k+1)}\n",
    ":=\n",
    "\\frac{ m_{t,k}/\\sigma_{t_0}^2 + \\tilde t_0/s_{\\bar t_0}^2 }\n",
    "     { 1/\\sigma_{t_0}^2 + 1/s_{\\bar t_0}^2 }.\n",
    "$$\n",
    "\n",
    "The computation for $\\bar v_0$ is identical, replacing $t_0$ by $v_0$ and using the priors\n",
    "$$\n",
    "v_0 \\mid \\bar v_0 \\sim \\mathcal N(\\bar v_0,\\sigma_{v_0}^2),\n",
    "\\qquad\n",
    "\\bar v_0 \\sim \\mathcal N(\\tilde v_0,s_{\\bar v_0}^2),\n",
    "$$\n",
    "with\n",
    "$$\n",
    "m_{v,k} = \\mathbb E[v_0 \\mid y,\\theta^{(k)}],\n",
    "$$\n",
    "which yields\n",
    "$$\n",
    "\\bar v_0^{(k+1)}\n",
    ":=\n",
    "\\frac{ m_{v,k}/\\sigma_{v_0}^2 + \\tilde v_0/s_{\\bar v_0}^2 }\n",
    "     { 1/\\sigma_{v_0}^2 + 1/s_{\\bar v_0}^2 }.\n",
    "$$\n",
    "\n",
    "\n",
    "Putting everything together, the HM–SAEM iteration is:\n",
    "\n",
    "1. Simulation (HM step): sample $z^{(k+1)}$ from an MH kernel targeting $p(z\\mid y,\\theta^{(k)})$.\n",
    "2. Stochastic approximation:\n",
    "   $$\n",
    "   S_{k+1} = S_k + \\varepsilon_k\\bigl(S(y,z^{(k+1)})-S_k\\bigr).\n",
    "   $$\n",
    "3. Maximization (MAP step): update $\\theta^{(k+1)}$ using\n",
    "   $$\n",
    "   \\sigma^{2,(k+1)} = \\frac{S_{1,k+1}+v^2}{K+m+2},\\quad\n",
    "   \\sigma_{\\xi}^{2,(k+1)} = \\frac{S_{2,k+1}+v_\\xi^2}{N+m_\\xi+2},\\quad\n",
    "   \\sigma_{\\tau}^{2,(k+1)} = \\frac{S_{3,k+1}+v_\\tau^2}{N+m_\\tau+2},\\quad\n",
    "   \\bar v_0^{(k+1)} = \\frac{ m_{v,k}/\\sigma_{v_0}^2 + \\tilde v_0/s_{\\bar v_0}^2 }{ 1/\\sigma_{v_0}^2 + 1/s_{\\bar v_0}^2 },\\quad\n",
    "   \\bar t_0^{(k+1)} = \\frac{ m_{t,k}/\\sigma_{t_0}^2 + \\tilde t_0/s_{\\bar t_0}^2 }{ 1/\\sigma_{t_0}^2 + 1/s_{\\bar t_0}^2 }.\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 : \n",
    "\n",
    "For individual $i$, we want to sample from the conditional posterior  \n",
    "$$\n",
    "p(z_i \\mid z_{\\text{pop}}, y, \\theta)\n",
    "= p(\\xi_i,\\tau_i \\mid t_0, v_0, y_i, \\theta).\n",
    "$$\n",
    "\n",
    "The log density of the conditional posterior is   \n",
    "$$\n",
    "\\log \\pi_i(\\xi_i,\\tau_i)\n",
    "=\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{j=1}^{k_i}\n",
    "\\big(\n",
    "y_{i,j} - p_0 - v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i)\n",
    "\\big)^2\n",
    "-\\frac{\\xi_i^2}{2\\sigma_\\xi^2}\n",
    "-\\frac{\\tau_i^2}{2\\sigma_\\tau^2}\n",
    "+ C,\n",
    "$$\n",
    "where $C$ does not depend on $(\\xi_i,\\tau_i)$.\n",
    "\n",
    "Because this conditional posterior is not of any standard form, it cannot be sampled directly.  \n",
    "We therefore update $z_i$ using a Metropolis–Hastings step inside the Gibbs sampler.\n",
    "\n",
    "1. Current state  \n",
    "$$\n",
    "z_i^{(k)} = (\\xi_i^{(k)},\\tau_i^{(k)}).\n",
    "$$\n",
    "\n",
    "2. Proposal (random walk)  \n",
    "$$\n",
    "z_i^* \\sim \\mathcal N\\big(z_i^{(k)}, \\Sigma_q\\big),\n",
    "$$\n",
    "where $\\Sigma_q$ is a $2 \\times 2$ proposal covariance matrix.\n",
    "\n",
    "3. Acceptance probability  \n",
    "$$\n",
    "\\alpha_i\n",
    "=\n",
    "\\min\\Big(\n",
    "1,\\;\n",
    "\\exp\\big[\n",
    "\\log \\pi_i(z_i^*) - \\log \\pi_i(z_i^{(k)})\n",
    "\\big]\n",
    "\\Big).\n",
    "$$\n",
    "\n",
    "4. Accept / reject (with $u \\sim \\mathcal U(0,1)$)  \n",
    "\n",
    "If $u \\le \\alpha_i $, set  \n",
    "$$\n",
    "z_i^{(k+1)} = z_i^* .\n",
    "$$\n",
    "\n",
    "Otherwise, set  \n",
    "$$\n",
    "z_i^{(k+1)} = z_i^{(k)} .\n",
    "$$\n",
    "\n",
    "In the Gibbs sampler, this Metropolis–Hastings update is applied sequentially  \n",
    "for $i = 1,\\dots,N$, conditioning on the current values of all other variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : \n",
    "\n",
    "We now consider the population effect\n",
    "$$\n",
    "z_{\\text{pop}} = (t_0,v_0)\n",
    "$$\n",
    "and we want to sample from the conditional posterior\n",
    "$$\n",
    "p(z_{\\text{pop}} \\mid z_1,\\dots,z_N, y, \\theta)\n",
    "= p(t_0,v_0 \\mid \\{\\xi_i,\\tau_i\\}_{i=1}^N, y, \\theta).\n",
    "$$\n",
    "\n",
    "Given the latent individual effects \\(z_i = (\\xi_i,\\tau_i)\\), the observation model is  \n",
    "$$\n",
    "y_{i,j} = p_0 + v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i) + \\varepsilon_{i,j},\n",
    "\\qquad\n",
    "\\varepsilon_{i,j} \\sim \\mathcal N(0,\\sigma^2),\n",
    "$$\n",
    "and the priors on the population parameters are\n",
    "$$\n",
    "t_0 \\sim \\mathcal N(\\bar t_0,\\sigma_{t_0}^2),\n",
    "\\qquad\n",
    "v_0 \\sim \\mathcal N(\\bar v_0,\\sigma_{v_0}^2),\n",
    "$$\n",
    "independently.\n",
    "\n",
    "Using Bayes’ formula and dropping constants that do not depend on \\((t_0,v_0)\\), the conditional posterior density is, up to a multiplicative constant,\n",
    "$$\n",
    "\\pi_{\\text{pop}}(t_0,v_0)\n",
    "\\propto\n",
    "p(y \\mid t_0,v_0,\\{z_i\\},\\theta)\\,\n",
    "p(t_0)\\,p(v_0).\n",
    "$$\n",
    "Its log-density can be written as\n",
    "$$\n",
    "\\log \\pi_{\\text{pop}}(t_0,v_0)\n",
    "=\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^N \\sum_{j=1}^{k_i}\n",
    "\\Big(\n",
    "y_{i,j} - p_0 - v_0 e^{\\xi_i}(t_{i,j}-t_0-\\tau_i)\n",
    "\\Big)^2\n",
    "-\\frac{(t_0-\\bar t_0)^2}{2\\sigma_{t_0}^2}\n",
    "-\\frac{(v_0-\\bar v_0)^2}{2\\sigma_{v_0}^2}\n",
    "+ C,\n",
    "$$\n",
    "where \\(C\\) does not depend on \\((t_0,v_0)\\).\n",
    "\n",
    "This conditional posterior has no closed form either, so we update \\(z_{\\text{pop}}\\) by a Metropolis–Hastings step within the Gibbs sampler.\n",
    "\n",
    "1. Current state  \n",
    "$$\n",
    "z_{\\text{pop}}^{(k)} = \\big(t_0^{(k)}, v_0^{(k)}\\big).\n",
    "$$\n",
    "\n",
    "2. Proposal (random walk)  \n",
    "$$\n",
    "z_{\\text{pop}}^* \\sim \\mathcal N\\big(z_{\\text{pop}}^{(k)}, \\Sigma_{\\text{pop}}\\big),\n",
    "$$\n",
    "where \\(\\Sigma_{\\text{pop}}\\) is a \\(2 \\times 2\\) proposal covariance matrix.\n",
    "\n",
    "3. Acceptance probability  \n",
    "$$\n",
    "\\alpha_{\\text{pop}}\n",
    "=\n",
    "\\min\\Big(\n",
    "1,\\;\n",
    "\\exp\\big[\n",
    "\\log \\pi_{\\text{pop}}(z_{\\text{pop}}^*)\n",
    "-\n",
    "\\log \\pi_{\\text{pop}}(z_{\\text{pop}}^{(k)})\n",
    "\\big]\n",
    "\\Big).\n",
    "$$\n",
    "\n",
    "4. Accept / reject (with $u \\sim \\mathcal U(0,1)$).  \n",
    "\n",
    "If $u \\le \\alpha_{\\text{pop}}$, set  \n",
    "$$\n",
    "z_{\\text{pop}}^{(k+1)} = z_{\\text{pop}}^* .\n",
    "$$\n",
    "\n",
    "Otherwise, set  \n",
    "$$\n",
    "z_{\\text{pop}}^{(k+1)} = z_{\\text{pop}}^{(k)} .\n",
    "$$\n",
    "\n",
    "In the full HMwG sampler, this update of \\(z_{\\text{pop}}\\) is combined with the Metropolis–Hastings updates of the individual effects \\(z_i\\), each step targeting the corresponding conditional posterior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7 : \n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Log-posteriors for MH steps\n",
    "# ---------------------------------------------\n",
    "\n",
    "def log_pi_i(xi, tau, i, t, y, theta):\n",
    "    \"\"\"\n",
    "    Log posterior (up to a constant) for z_i = (xi_i, tau_i),\n",
    "    given (t0, v0, theta) and data of individual i.\n",
    "    \"\"\"\n",
    "    t0      = theta[\"t0\"]\n",
    "    v0      = theta[\"v0\"]\n",
    "    sigma2  = theta[\"sigma2\"]\n",
    "    sig_xi2 = theta[\"sigma_xi2\"]\n",
    "    sig_tau2= theta[\"sigma_tau2\"]\n",
    "    p0      = theta[\"p0\"]\n",
    "\n",
    "    t_i = t[i]        # shape (k,)\n",
    "    y_i = y[i]        # shape (k,)\n",
    "\n",
    "    mu_i = p0 + v0 * np.exp(xi) * (t_i - t0 - tau)\n",
    "    ll   = -0.5 / sigma2 * np.sum((y_i - mu_i)**2)              # likelihood\n",
    "    lp_xi  = -0.5 * xi**2  / sig_xi2                            # prior on xi_i\n",
    "    lp_tau = -0.5 * tau**2 / sig_tau2                           # prior on tau_i\n",
    "    return ll + lp_xi + lp_tau\n",
    "\n",
    "\n",
    "def log_pi_pop(t0, v0, z, t, y, theta):\n",
    "    \"\"\"\n",
    "    Log posterior (up to a constant) for z_pop = (t0, v0),\n",
    "    given all individual effects z = [(xi_i, tau_i)].\n",
    "    \"\"\"\n",
    "    sigma2   = theta[\"sigma2\"]\n",
    "    p0       = theta[\"p0\"]\n",
    "    tbar0    = theta[\"tbar0\"]\n",
    "    s_t0_2   = theta[\"s_t0\"]**2\n",
    "    vbar0    = theta[\"vbar0\"]\n",
    "    s_v0_2   = theta[\"s_v0\"]**2\n",
    "\n",
    "    ll = 0.0\n",
    "    for i, (xi_i, tau_i) in enumerate(z):\n",
    "        t_i = t[i]\n",
    "        y_i = y[i]\n",
    "        mu_i = p0 + v0 * np.exp(xi_i) * (t_i - t0 - tau_i)\n",
    "        ll += -0.5 / sigma2 * np.sum((y_i - mu_i)**2)\n",
    "\n",
    "    lp_t0 = -0.5 * (t0 - tbar0)**2 / s_t0_2\n",
    "    lp_v0 = -0.5 * (v0 - vbar0)**2 / s_v0_2\n",
    "    return ll + lp_t0 + lp_v0\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. One HM-within-Gibbs sweep on z\n",
    "# ---------------------------------------------\n",
    "\n",
    "def mh_update_individuals(z_pop, z, theta, t, y, Sigma_q_ind):\n",
    "    \"\"\"\n",
    "    One MH-within-Gibbs sweep over all z_i = (xi_i, tau_i).\n",
    "    z : array of shape (N, 2)\n",
    "    \"\"\"\n",
    "    theta_loc = theta.copy()\n",
    "    theta_loc[\"t0\"], theta_loc[\"v0\"] = z_pop\n",
    "\n",
    "    N = z.shape[0]\n",
    "    for i in range(N):\n",
    "        xi_curr, tau_curr = z[i]\n",
    "\n",
    "        # proposal\n",
    "        prop = np.random.multivariate_normal(\n",
    "            mean=np.array([xi_curr, tau_curr]),\n",
    "            cov=Sigma_q_ind\n",
    "        )\n",
    "        xi_prop, tau_prop = prop\n",
    "\n",
    "        log_curr = log_pi_i(xi_curr, tau_curr, i, t, y, theta_loc)\n",
    "        log_prop = log_pi_i(xi_prop, tau_prop, i, t, y, theta_loc)\n",
    "\n",
    "        alpha = min(1.0, np.exp(log_prop - log_curr))\n",
    "        if np.random.rand() <= alpha:\n",
    "            z[i] = np.array([xi_prop, tau_prop])\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def mh_update_pop(z_pop, z, theta, t, y, Sigma_q_pop):\n",
    "    \"\"\"\n",
    "    One MH update for z_pop = (t0, v0).\n",
    "    \"\"\"\n",
    "    t0_curr, v0_curr = z_pop\n",
    "\n",
    "    log_curr = log_pi_pop(t0_curr, v0_curr, z, t, y, theta)\n",
    "\n",
    "    prop = np.random.multivariate_normal(\n",
    "        mean=np.array([t0_curr, v0_curr]),\n",
    "        cov=Sigma_q_pop\n",
    "    )\n",
    "    t0_prop, v0_prop = prop\n",
    "\n",
    "    log_prop = log_pi_pop(t0_prop, v0_prop, z, t, y, theta)\n",
    "\n",
    "    alpha = min(1.0, np.exp(log_prop - log_curr))\n",
    "    if np.random.rand() <= alpha:\n",
    "        return np.array([t0_prop, v0_prop])\n",
    "    else:\n",
    "        return np.array([t0_curr, v0_curr])\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Sufficient statistics S1..S4\n",
    "# ---------------------------------------------\n",
    "\n",
    "def compute_sufficient_stats(z_pop, z, t, y, theta):\n",
    "    \"\"\"\n",
    "    Returns S = (S1, S2, S3, S4) for given latent variables.\n",
    "    \"\"\"\n",
    "    t0, v0 = z_pop\n",
    "    p0     = theta[\"p0\"]\n",
    "\n",
    "    S1 = 0.0   # sum of squared residuals\n",
    "    S2 = 0.0   # sum xi_i^2\n",
    "    S3 = 0.0   # sum tau_i^2\n",
    "    K  = 0     # total number of observations\n",
    "\n",
    "    for (xi_i, tau_i), t_i, y_i in zip(z, t, y):\n",
    "        mu_i = p0 + v0 * np.exp(xi_i) * (t_i - t0 - tau_i)\n",
    "        S1 += np.sum((y_i - mu_i)**2)\n",
    "        S2 += xi_i**2\n",
    "        S3 += tau_i**2\n",
    "        K  += len(y_i)\n",
    "\n",
    "    S4 = K\n",
    "    return np.array([S1, S2, S3, S4])\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. HMwG–SAEM main loop\n",
    "# ---------------------------------------------\n",
    "\n",
    "def hmwg_saem(t, y,\n",
    "              n_iter=1000, N_burn=200, alpha=0.8,\n",
    "              Sigma_q_ind=None, Sigma_q_pop=None):\n",
    "    \"\"\"\n",
    "    HM-within-Gibbs SAEM to approximate the MAP of\n",
    "    (sigma2, sigma_xi2, sigma_tau2, t0, v0) with given priors.\n",
    "    Uses the global simulation hyperparameters already defined above.\n",
    "    \"\"\"\n",
    "\n",
    "    N, k = y.shape\n",
    "\n",
    "    # --- initial parameters / priors ---\n",
    "    theta = {\n",
    "        \"p0\": p0,\n",
    "        \"t0\": 0.0,\n",
    "        \"v0\": 1.0,\n",
    "        \"sigma2\": 1.0,\n",
    "        \"sigma_xi2\": 1.0,\n",
    "        \"sigma_tau2\": 1.0,\n",
    "        \"tbar0\": tbar0,\n",
    "        \"s_t0\": s_t0,\n",
    "        \"vbar0\": vbar0,\n",
    "        \"s_v0\": s_v0,\n",
    "        # hyperparameters for inverse-Wishart (can be tuned)\n",
    "        \"v_eps\": 1.0,   \"m_eps\": 3.0,\n",
    "        \"v_xi\":  1.0,   \"m_xi\": 3.0,\n",
    "        \"v_tau\": 1.0,   \"m_tau\":3.0,\n",
    "    }\n",
    "\n",
    "    # initial latent variables\n",
    "    z_pop = np.array([theta[\"t0\"], theta[\"v0\"]])\n",
    "    z     = np.random.normal(0.0, 1.0, size=(N, 2))  # (xi_i, tau_i)\n",
    "\n",
    "    # proposals\n",
    "    if Sigma_q_ind is None:\n",
    "        Sigma_q_ind = 0.05 * np.eye(2)\n",
    "    if Sigma_q_pop is None:\n",
    "        Sigma_q_pop = 0.05 * np.eye(2)\n",
    "\n",
    "    # initial sufficient stats\n",
    "    S = compute_sufficient_stats(z_pop, z, t, y, theta)\n",
    "\n",
    "    theta_path = []\n",
    "\n",
    "    for k_iter in range(n_iter):\n",
    "\n",
    "        # step-size epsilon_k\n",
    "        if k_iter < N_burn:\n",
    "            eps_k = 1.0\n",
    "        else:\n",
    "            eps_k = (k_iter - N_burn + 1) ** (-alpha)\n",
    "\n",
    "        # 1) Simulation: one HMwG sweep\n",
    "        z     = mh_update_individuals(z_pop, z, theta, t, y, Sigma_q_ind)\n",
    "        z_pop = mh_update_pop(z_pop, z, theta, t, y, Sigma_q_pop)\n",
    "\n",
    "        # 2) Stochastic approximation of S\n",
    "        S_new = compute_sufficient_stats(z_pop, z, t, y, theta)\n",
    "        S = S + eps_k * (S_new - S)\n",
    "        S1, S2, S3, S4 = S\n",
    "        K_tot = S4\n",
    "\n",
    "        # 3) Maximization step (MAP updates: formulas of Question 4)\n",
    "        # variances with inverse-Wishart priors\n",
    "        theta[\"sigma2\"]      = (S1 + theta[\"v_eps\"]**2) / (K_tot + theta[\"m_eps\"] + 2)\n",
    "        theta[\"sigma_xi2\"]   = (S2 + theta[\"v_xi\"]**2)   / (N     + theta[\"m_xi\"]  + 2)\n",
    "        theta[\"sigma_tau2\"]  = (S3 + theta[\"v_tau\"]**2)  / (N     + theta[\"m_tau\"] + 2)\n",
    "\n",
    "        # here we simply keep z_pop as current estimate of (t0,v0)\n",
    "        theta[\"t0\"], theta[\"v0\"] = z_pop\n",
    "\n",
    "        theta_path.append(theta.copy())\n",
    "\n",
    "    return theta, z_pop, z, theta_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/q9qyh7fs6kdg4p3k63s7zk5r0000gn/T/ipykernel_2987/954347747.py:79: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = min(1.0, np.exp(log_prop - log_curr))\n",
      "/var/folders/r7/q9qyh7fs6kdg4p3k63s7zk5r0000gn/T/ipykernel_2987/954347747.py:102: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = min(1.0, np.exp(log_prop - log_curr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True population parameters:\n",
      "  t0 (true) = -1.737\n",
      "  v0 (true) = 0.832\n",
      "\n",
      "Estimated population parameters (end of SAEM):\n",
      "  t0_hat    = -1.611\n",
      "  v0_hat    = 0.859\n",
      "\n",
      "True variances:\n",
      "  sigma^2       (true) = 0.250\n",
      "  sigma_xi^2    (true) = 0.200\n",
      "  sigma_tau^2   (true) = 1.000\n",
      "\n",
      "Estimated variances (end of SAEM):\n",
      "  sigma^2_hat       = 0.248\n",
      "  sigma_xi^2_hat    = 0.263\n",
      "  sigma_tau^2_hat   = 0.983\n"
     ]
    }
   ],
   "source": [
    "# lancer l'HMwG–SAEM sur les données simulées (t, y)\n",
    "theta_hat, z_pop_hat, z_hat, theta_path = hmwg_saem(\n",
    "    t, y,\n",
    "    n_iter=2000,     \n",
    "    N_burn=500,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "print(\"True population parameters:\")\n",
    "print(f\"  t0 (true) = {t0:.3f}\")\n",
    "print(f\"  v0 (true) = {v0:.3f}\")\n",
    "print()\n",
    "print(\"Estimated population parameters (end of SAEM):\")\n",
    "print(f\"  t0_hat    = {z_pop_hat[0]:.3f}\")\n",
    "print(f\"  v0_hat    = {z_pop_hat[1]:.3f}\")\n",
    "print()\n",
    "print(\"True variances:\")\n",
    "print(f\"  sigma^2       (true) = {sigma2:.3f}\")\n",
    "print(f\"  sigma_xi^2    (true) = {sigma_xi2:.3f}\")\n",
    "print(f\"  sigma_tau^2   (true) = {sigma_tau2:.3f}\")\n",
    "print()\n",
    "print(\"Estimated variances (end of SAEM):\")\n",
    "print(f\"  sigma^2_hat       = {theta_hat['sigma2']:.3f}\")\n",
    "print(f\"  sigma_xi^2_hat    = {theta_hat['sigma_xi2']:.3f}\")\n",
    "print(f\"  sigma_tau^2_hat   = {theta_hat['sigma_tau2']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters are close enough to the true ones. The SAEM algorithm is efficient on our datas. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 : \n",
    "\n",
    "A Block Gibbs sampler updates several latent variables at once instead of\n",
    "one by one.  \n",
    "In our model, this has two main advantages:\n",
    "\n",
    "- **Faster mixing**: the random effects are correlated through the\n",
    "  population parameters. Updating them jointly allows the chain to move\n",
    "  more efficiently and reduces autocorrelation.\n",
    "\n",
    "- **Better scaling with large N**: when the number of individuals is\n",
    "  large, one-at-a-time updates become very slow. Block updates make the\n",
    "  sampler explore the posterior much faster.\n",
    "\n",
    "Overall, block sampling improves convergence and is more efficient on\n",
    "large datasets.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9 : \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Block log-posterior for all individual effects z_{1:N} ---\n",
    "\n",
    "def log_pi_block_individuals(z, z_pop, theta, t, y):\n",
    "    \"\"\"\n",
    "    Joint log-posterior (up to a constant) for all individual effects\n",
    "    z = array of shape (N, 2) with rows (xi_i, tau_i),\n",
    "    given z_pop = (t0, v0), data (t,y), and parameters theta.\n",
    "    \"\"\"\n",
    "    t0, v0 = z_pop\n",
    "    theta_loc = theta.copy()\n",
    "    theta_loc[\"t0\"], theta_loc[\"v0\"] = t0, v0\n",
    "\n",
    "    N = z.shape[0]\n",
    "    out = 0.0\n",
    "    for i in range(N):\n",
    "        xi_i, tau_i = z[i]\n",
    "        out += log_pi_i(xi_i, tau_i, i, t, y, theta_loc)\n",
    "    return out\n",
    "\n",
    "\n",
    "# --- Block MH update for all individuals z_{1:N} ---\n",
    "\n",
    "def block_mh_update_individuals(z_pop, z, theta, t, y, Sigma_block):\n",
    "    \"\"\"\n",
    "    One Metropolis-Hastings update of the whole block z_{1:N}.\n",
    "\n",
    "    z_pop : current (t0, v0), shape (2,)\n",
    "    z     : current individuals, shape (N, 2)\n",
    "    Sigma_block : (2N x 2N) covariance matrix for the proposal.\n",
    "    \"\"\"\n",
    "    N = z.shape[0]\n",
    "    z_vec = z.reshape(2 * N)  # flatten to 1D\n",
    "\n",
    "    log_curr = log_pi_block_individuals(z, z_pop, theta, t, y)\n",
    "\n",
    "    # proposal on the full 2N-dimensional vector\n",
    "    z_vec_prop = np.random.multivariate_normal(mean=z_vec, cov=Sigma_block)\n",
    "    z_prop = z_vec_prop.reshape(N, 2)\n",
    "\n",
    "    log_prop = log_pi_block_individuals(z_prop, z_pop, theta, t, y)\n",
    "\n",
    "    alpha = min(1.0, np.exp(log_prop - log_curr))\n",
    "    if np.random.rand() <= alpha:\n",
    "        return z_prop  # accept\n",
    "    else:\n",
    "        return z       # reject\n",
    "\n",
    "\n",
    "# --- Block MH update for population effects (t0, v0) ---\n",
    "\n",
    "def block_mh_update_pop(z_pop, z, theta, t, y, Sigma_pop):\n",
    "    \"\"\"\n",
    "    One Metropolis-Hastings update of the block z_pop = (t0, v0).\n",
    "    \"\"\"\n",
    "    t0_curr, v0_curr = z_pop\n",
    "    log_curr = log_pi_pop(t0_curr, v0_curr, z, t, y, theta)\n",
    "\n",
    "    prop = np.random.multivariate_normal(mean=z_pop, cov=Sigma_pop)\n",
    "    t0_prop, v0_prop = prop\n",
    "    log_prop = log_pi_pop(t0_prop, v0_prop, z, t, y, theta)\n",
    "\n",
    "    alpha = min(1.0, np.exp(log_prop - log_curr))\n",
    "    if np.random.rand() <= alpha:\n",
    "        return np.array([t0_prop, v0_prop])\n",
    "    else:\n",
    "        return z_pop\n",
    "\n",
    "\n",
    "# --- Full Block HMwG sampler (without SAEM), just to illustrate ---\n",
    "\n",
    "def block_hmwg_sampler(t, y, theta_init,\n",
    "                       n_iter=2000,\n",
    "                       Sigma_block_ind_scale=0.05,\n",
    "                       Sigma_pop_scale=0.05):\n",
    "    \"\"\"\n",
    "    Simple Block HMwG sampler:\n",
    "\n",
    "    - Block 1: z_pop = (t0, v0)\n",
    "    - Block 2: z_{1:N} = {(xi_i, tau_i)}_{i=1}^N\n",
    "    \"\"\"\n",
    "\n",
    "    theta = theta_init.copy()\n",
    "\n",
    "    N = y.shape[0]\n",
    "\n",
    "    # initial latent variables\n",
    "    z_pop = np.array([theta[\"t0\"], theta[\"v0\"]])\n",
    "    z     = np.random.normal(0.0, 1.0, size=(N, 2))  # (xi_i, tau_i)\n",
    "\n",
    "    # proposal covariances\n",
    "    Sigma_block_ind = (Sigma_block_ind_scale**2) * np.eye(2 * N)\n",
    "    Sigma_pop       = (Sigma_pop_scale**2)       * np.eye(2)\n",
    "\n",
    "    # to store a few samples if needed\n",
    "    z_pop_chain = np.zeros((n_iter, 2))\n",
    "    z_chain     = np.zeros((n_iter, N, 2))\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        # Block 1: update population effects\n",
    "        z_pop = block_mh_update_pop(z_pop, z, theta, t, y, Sigma_pop)\n",
    "\n",
    "        # Block 2: update all individuals jointly\n",
    "        z = block_mh_update_individuals(z_pop, z, theta, t, y, Sigma_block_ind)\n",
    "\n",
    "        z_pop_chain[it] = z_pop\n",
    "        z_chain[it]     = z\n",
    "\n",
    "    return z_pop_chain, z_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/q9qyh7fs6kdg4p3k63s7zk5r0000gn/T/ipykernel_2987/3772162734.py:46: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = min(1.0, np.exp(log_prop - log_curr))\n",
      "/var/folders/r7/q9qyh7fs6kdg4p3k63s7zk5r0000gn/T/ipykernel_2987/3772162734.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = min(1.0, np.exp(log_prop - log_curr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical mean of t0, v0 under block HMwG:\n",
      "[-1.88612848  0.78475192]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d'utilisation du sampler bloc \n",
    "theta_init = {\n",
    "    \"p0\": p0,\n",
    "    \"t0\": tbar0,     \n",
    "    \"v0\": vbar0,\n",
    "    \"sigma2\": sigma2,\n",
    "    \"sigma_xi2\": sigma_xi2,\n",
    "    \"sigma_tau2\": sigma_tau2,\n",
    "    \"tbar0\": tbar0,\n",
    "    \"s_t0\": s_t0,\n",
    "    \"vbar0\": vbar0,\n",
    "    \"s_v0\": s_v0,\n",
    "}\n",
    "\n",
    "z_pop_chain, z_chain = block_hmwg_sampler(\n",
    "    t, y, theta_init,\n",
    "    n_iter=2000,\n",
    "    Sigma_block_ind_scale=0.05,\n",
    "    Sigma_pop_scale=0.05\n",
    ")\n",
    "\n",
    "print(\"Empirical mean of t0, v0 under block HMwG:\")\n",
    "print(z_pop_chain.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2 : \n",
    "\n",
    "### Q1 : \n",
    "\n",
    "\n",
    "The algorithm performs the update  \n",
    "$Y_n \\rightarrow X_{n+1} \\sim f_{X|Y}(\\cdot \\mid Y_n)$  \n",
    "and then  \n",
    "$X_{n+1} \\rightarrow Y_{n+1} \\sim f_{Y|X}(\\cdot \\mid X_{n+1})$.\n",
    "\n",
    "Let $A \\subset \\mathbb{R}^p$ and $B \\subset \\mathbb{R}^q$.  \n",
    "We want the transition probability\n",
    "$$\n",
    "\\mathbb{P}\\big((X_{n+1},Y_{n+1}) \\in A\\times B \\,\\big|\\, X_0,Y_0,\\dots,X_n,Y_n\\big).\n",
    "$$\n",
    "\n",
    "Since  \n",
    "$X_{n+1}\\mid Y_n \\sim f_{X|Y}(\\cdot \\mid Y_n)$  \n",
    "and  \n",
    "$Y_{n+1}\\mid X_{n+1}=x' \\sim f_{Y|X}(\\cdot \\mid x')$,\n",
    "\n",
    "Then :\n",
    "$$\n",
    "\\mathbb{P}\\big((X_{n+1},Y_{n+1})\\in A\\times B \\mid X_0,\\dots,Y_n\\big)\n",
    "=\n",
    "\\int_A \\int_B \n",
    "f_{X|Y}(x' \\mid Y_n)\\, f_{Y|X}(y' \\mid x') \\, dy'\\, dx'.\n",
    "$$\n",
    "This only depends of $(X_n,Y_n)$ which proves that the bivariate chain is a Markov chain ! \n",
    "Thus the transition kernel is : \n",
    "$$\n",
    "K((x,y),A\\times B)\n",
    "=\n",
    "\\int_A \\int_B \n",
    "f_{X|Y}(x' \\mid y)\\, f_{Y|X}(y' \\mid x') \\, dy'\\, dx'.\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to prove that $(Y_n)_{n\\ge0}$ is Markov, i.e.\n",
    "$$\n",
    "\\mathbb{P}(Y_{n+1}\\in B \\mid Y_0,\\dots,Y_n)\n",
    "=\n",
    "\\mathbb{P}(Y_{n+1}\\in B \\mid Y_n)\n",
    "$$\n",
    "for any measurable $B\\subset\\mathbb{R}^q$.\n",
    "\n",
    "Start from\n",
    "$$\n",
    "\\mathbb{P}(Y_{n+1}\\in B \\mid Y_0,\\dots,Y_n)\n",
    "=\n",
    "\\mathbb{E}\\big[ \\mathbf{1}_{\\{Y_{n+1}\\in B\\}} \\mid Y_0,\\dots,Y_n \\big].\n",
    "$$\n",
    "\n",
    "Use the tower property with the following $\\sigma$–field  \n",
    "$\\mathcal{F}_n := \\sigma(X_0,Y_0,\\dots,X_n,Y_n)$:\n",
    "$$\n",
    "\\mathbb{E}\\big[ \\mathbf{1}_{\\{Y_{n+1}\\in B\\}} \\mid Y_0,\\dots,Y_n \\big]\n",
    "=\n",
    "\\mathbb{E}\\Big[\n",
    "  \\mathbb{E}\\big[ \\mathbf{1}_{\\{Y_{n+1}\\in B\\}} \\mid \\mathcal{F}_n \\big]\n",
    "  \\,\\Big|\\, Y_0,\\dots,Y_n\n",
    "\\Big].\n",
    "$$\n",
    "\n",
    "Because $\\big((X_k,Y_k)\\big)_{k\\ge0}$ is a Markov chain, we have\n",
    "$$\n",
    "\\mathbb{E}\\big[ \\mathbf{1}_{\\{Y_{n+1}\\in B\\}} \\mid \\mathcal{F}_n \\big]\n",
    "=\n",
    "\\mathbb{P}(Y_{n+1}\\in B \\mid X_n,Y_n)\n",
    "=: h(X_n,Y_n).\n",
    "$$\n",
    "\n",
    "From the explicit transition kernel of the bivariate chain (question 1),\n",
    "$$\n",
    "h(x,y)\n",
    "=\n",
    "\\int_{\\mathbb{R}^p}\\int_B f_{X\\mid Y}(x' \\mid y)\\,f_{Y\\mid X}(y' \\mid x')\\,dy'\\,dx'\n",
    "=:\\tilde h(y),\n",
    "$$\n",
    "so in fact $h(x,y)$ does **not** depend on $x$, only on $y$.\n",
    "\n",
    "Thus\n",
    "$$\n",
    "\\mathbb{P}(Y_{n+1}\\in B \\mid Y_0,\\dots,Y_n)\n",
    "=\n",
    "\\mathbb{E}\\big[ \\tilde h(Y_n) \\mid Y_0,\\dots,Y_n \\big]\n",
    "=\n",
    "\\tilde h(Y_n),\n",
    "$$\n",
    "because $\\tilde h(Y_n)$ is $\\sigma(Y_n)$-measurable and\n",
    "$\\sigma(Y_n)\\subset\\sigma(Y_0,\\dots,Y_n)$.\n",
    "\n",
    "Finally,\n",
    "$$\n",
    "\\tilde h(Y_n)\n",
    "=\n",
    "\\mathbb{P}(Y_{n+1}\\in B \\mid Y_n),\n",
    "$$\n",
    "which proves that $(Y_n)_{n\\ge0}$ is a Markov chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the transition kernel is \n",
    "$$\n",
    "K_Y(y,B)\n",
    "=\n",
    "\\int_{\\mathbb{R}^p}\\int_B f_{X\\mid Y}(x' \\mid y)\\,\n",
    "                         f_{Y\\mid X}(y' \\mid x')\\,dy'\\,dx'.\n",
    "$$\n",
    "\n",
    "Equivalently, it admits a transition density \n",
    "$$\n",
    "k_Y(y,y') = \\int_{\\mathbb{R}^p} f_{X\\mid Y}(x' \\mid y)\\,f_{Y\\mid X}(y' \\mid x')\\,dx'. \n",
    "$$\n",
    "\n",
    "\n",
    "To prove that $f_Y(y)\\,dy$ is invariant for $K_Y$, compute the image\n",
    "density:\n",
    "$$\n",
    "\\int_{\\mathbb{R}^q} f_Y(y)\\,k_Y(y,y')\\,dy\n",
    "=\n",
    "\\int_{\\mathbb{R}^q} f_Y(y)\n",
    "\\left[\n",
    "\\int_{\\mathbb{R}^p} f_{X\\mid Y}(x \\mid y)\\,f_{Y\\mid X}(y' \\mid x)\\,dx\n",
    "\\right] dy.\n",
    "$$\n",
    "Using\n",
    "$$\n",
    "f_{X\\mid Y}(x \\mid y) = \\frac{f(x,y)}{f_Y(y)}, \n",
    "\\qquad\n",
    "f_{Y\\mid X}(y' \\mid x) = \\frac{f(x,y')}{f_X(x)},\n",
    "$$\n",
    "we get\n",
    "$$\n",
    "\\int_{\\mathbb{R}^q} f_Y(y)\\,k_Y(y,y')\\,dy\n",
    "=\n",
    "\\int_{\\mathbb{R}^q}\\int_{\\mathbb{R}^p}\n",
    "\\frac{f(x,y)}{f_Y(y)}\\,f_Y(y)\\,\n",
    "\\frac{f(x,y')}{f_X(x)}\\,dx\\,dy\n",
    "=\n",
    "\\int_{\\mathbb{R}^p}\n",
    "\\left[\\int_{\\mathbb{R}^q} f(x,y)\\,dy\\right]\n",
    "\\frac{f(x,y')}{f_X(x)}\\,dx.\n",
    "$$\n",
    "But $\\int_{\\mathbb{R}^q} f(x,y)\\,dy = f_X(x)$, so\n",
    "$$\n",
    "\\int_{\\mathbb{R}^q} f_Y(y)\\,k_Y(y,y')\\,dy\n",
    "=\n",
    "\\int_{\\mathbb{R}^p} f_X(x)\\,\\frac{f(x,y')}{f_X(x)}\\,dx\n",
    "=\n",
    "\\int_{\\mathbb{R}^p} f(x,y')\\,dx\n",
    "= f_Y(y').\n",
    "$$\n",
    "Therefore, for any measurable $B \\subset \\mathbb{R}^q$,\n",
    "$$\n",
    "\\int_{\\mathbb{R}^q} f_Y(y)\\,K_Y(y,B)\\,dy\n",
    "= \\int_B f_Y(y')\\,dy',\n",
    "$$\n",
    "so $f_Y(y)\\,dy$ is invariant for the transition kernel $K_Y$ : $K_Y f_Y(y)\\,dy=f_Y(y)\\,dy$ ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 : \n",
    "\n",
    "We want to simulate from the joint density \\(f(x,y)\\).  \n",
    "Assume we know the two full conditional densities\n",
    "\n",
    "- $f_{X\\mid Y}(x \\mid y)$  (here: a Gamma distribution in $x$),\n",
    "- $f_{Y\\mid X}(y \\mid x)$  (here: a Gamma distribution in $y$).\n",
    "\n",
    "A Gibbs sampler is:\n",
    "\n",
    "1. **Initialization.**  \n",
    "   Choose any starting point $(x^{(0)},y^{(0)})\\in\\mathbb{R}^2$.\n",
    "\n",
    "2. **For \\(n = 0,\\dots,N-1\\):**\n",
    "   - Sample\n",
    "     $$\n",
    "     X^{(n+1)} \\sim f_{X\\mid Y}(\\,\\cdot \\mid y^{(n)}) ,\n",
    "     $$\n",
    "     e.g. in Python  \n",
    "     `x[n+1] = np.random.gamma(shape_x(y[n]), scale_x(y[n]))`.\n",
    "   - Then sample\n",
    "     $$\n",
    "     Y^{(n+1)} \\sim f_{Y\\mid X}(\\,\\cdot \\mid x^{(n+1)}) ,\n",
    "     $$\n",
    "     e.g.  \n",
    "     `y[n+1] = np.random.gamma(shape_y(x[n+1]), scale_y(x[n+1]))`.\n",
    "\n",
    "3. **Output.**  \n",
    "   After discarding a burn–in period, the samples\n",
    "   $\\{(X^{(n)},Y^{(n)})\\}_{n=B}^N$ form an approximate sample from the\n",
    "   target distribution with density $f$. Expectations under $f$ can\n",
    "   be approximated by empirical averages\n",
    "   $$\n",
    "   \\frac{1}{N-B+1}\\sum_{n=B}^N g\\big(X^{(n)},Y^{(n)}\\big).\n",
    "   $$\n",
    "   where $g$ is any target map such that we aim to compute $\\mathbb{E}_f[g(X,Y)]$, here using MCMC methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4:\n",
    "\n",
    "We want to evaluate\n",
    "$$\n",
    "I=\\int_{\\mathbb{R}} \\frac{H(x)}{(4+x^2)^{5/2}}\\,dx.\n",
    "$$\n",
    "\n",
    "From Question 1, the bivariate process $(X_n,Y_n)$ produced by the Gibbs\n",
    "algorithm is a Markov chain with invariant density $f(x,y)$.  \n",
    "From Question 2, the marginal chain $(Y_n)$ is itself Markov with invariant\n",
    "density $f_Y(y)$.\n",
    "\n",
    "Since $X_n$ is a measurable function of $(X_n,Y_n)$, the sequence $(X_n)$ is\n",
    "also a Markov chain. Its invariant density is the marginal\n",
    "$$\n",
    "f_X(x)=\\int_{\\mathbb{R}} f(x,y)\\,dy.\n",
    "$$\n",
    "\n",
    "In the model of this exercise, we have\n",
    "$$\n",
    "f_X(x) \\propto \\frac{1}{(4+x^2)^{5/2}},\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "I = C\\,\\mathbb{E}_{f_X}[H(X)]\n",
    "$$\n",
    "for some normalizing constant $C$.\n",
    "\n",
    "Therefore, the Gibbs sampler provides samples\n",
    "$$\n",
    "X_B,X_{B+1},\\dots,X_N \\sim f_X\n",
    "$$\n",
    "after discarding a burn–in period $B$.  \n",
    "Hence a Monte Carlo approximation of the integral is\n",
    "$$\n",
    "\\widehat{I}_N\n",
    "= \\frac{1}{N-B+1}\\sum_{n=B}^N H(X_n).\n",
    "$$\n",
    "\n",
    "Up to the normalizing constant $C$, this converges to the desired integral by the\n",
    "ergodic theorem for Markov chains. This is a practical MCMC example ! \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 : \n",
    "\n",
    "We work on $(-1,1)$.  \n",
    "Given the current state $X=x$, we draw\n",
    "- $\\varepsilon \\sim f$ on $(-1,1)$,\n",
    "- $B \\sim \\mathcal{B}(1/2)$.\n",
    "\n",
    "If $B=1$ we set $Y=\\varepsilon x$, otherwise $Y = x/\\varepsilon$.  \n",
    "$Y$ is the proposal, with conditional distribution $q(x,\\cdot)$.\n",
    "\n",
    "\n",
    "For fixed $x\\in(-1,1)$ and $t\\in(-1,1)$, set\n",
    "$$\n",
    "Q_x(t) := \\mathbb P(Y < t \\mid X=x).\n",
    "$$\n",
    "We decompose with respect to $B$:\n",
    "$$\n",
    "Q_x(t)\n",
    "= \\mathbb P(Y<t,B=1\\mid X=x)+\\mathbb P(Y<t,B=0\\mid X=x).\n",
    "$$\n",
    "\n",
    "If $B=1$: $Y=\\varepsilon$\n",
    "\n",
    "Then\n",
    "$$\n",
    "Y<t \\iff \\varepsilon < \\frac{t}{x}.\n",
    "$$\n",
    "Let $F$ be the cdf of $\\varepsilon$.\n",
    "For this branch we get\n",
    "$$\n",
    "\\mathbb P(Y<t,B=1\\mid X=x)\n",
    "= \\frac12\\,F\\!\\left(\\frac{t}{x}\\right),\n",
    "$$\n",
    "valid only when $|t|<|x|$ (so that $t/x\\in(-1,1)$).\n",
    "\n",
    "Now if \\(B=0\\) : $Y=x/ \\varepsilon x$\n",
    "\n",
    "Then\n",
    "$$\n",
    "Y<t \\iff \\frac{x}{\\varepsilon}<t \\iff \\varepsilon > \\frac{x}{t}.\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "\\mathbb P(Y<t,B=0\\mid X=x)\n",
    "= \\frac12\\Big(1-F\\!\\left(\\frac{x}{t}\\right)\\Big),\n",
    "$$\n",
    "valid when $|t|>|x|$ (so that $x/t\\in(-1,1)$).\n",
    "\n",
    "Thus\n",
    "$$\n",
    "Q_x(t)=\n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac12\\,F\\!\\left(\\frac{t}{x}\\right), & |t|<|x|,\\\\[0.3cm]\n",
    "\\displaystyle \\frac12\\Big(1-F\\!\\left(\\frac{x}{t}\\right)\\Big), & |t|>|x|,\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The jumping density is\n",
    "$$\n",
    "q(x,y) = Q_x'(y).\n",
    "$$\n",
    "Using $F'=f$ and the chain rule:\n",
    "\n",
    "- for $|y|<|x|$,\n",
    "  $$\n",
    "  q(x,y) = \\frac12\\,f\\!\\left(\\frac{y}{x}\\right)\\,\\frac{1}{|x|};\n",
    "  $$\n",
    "- for $|x|<|y|<1$,\n",
    "  $$\n",
    "  q(x,y) = \\frac12\\,f\\!\\left(\\frac{x}{y}\\right)\\,\\frac{|x|}{y^2}.\n",
    "  $$\n",
    "\n",
    "So, for $x,y\\in(-1,1)$,\n",
    "$$\n",
    "q(x,y)\n",
    "=\n",
    "\\frac12\\,\\mathbf 1_{\\{|y|<|x|\\}}\\,\n",
    "      f\\!\\left(\\frac{y}{x}\\right)\\frac{1}{|x|}\n",
    "+\n",
    "\\frac12\\,\\mathbf 1_{\\{|x|<|y|<1\\}}\\,\n",
    "      f\\!\\left(\\frac{x}{y}\\right)\\frac{|x|}{y^{2}}.\n",
    "$$\n",
    "\n",
    "This $q(x,\\cdot)$ is the proposal kernel of the Markov chain\n",
    "(conditioning on the current state $X=x$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 : \n",
    "\n",
    "\n",
    "In Metropolis–Hastings, if we take\n",
    "$$\n",
    "\\alpha(x,y)\n",
    ":=\n",
    "1 \\wedge \\frac{\\pi(y)\\,q(y,x)}{\\pi(x)\\,q(x,y)}.\n",
    "$$\n",
    "The measure will necessarily be invariant !\n",
    "\n",
    "We now compute the ratio $q(y,x)/q(x,y)$.\n",
    "\n",
    "### Case A: $|y|<|x|$\n",
    "\n",
    "By the formula above,\n",
    "$$\n",
    "q(x,y) = \\frac12 f\\!\\left(\\frac{y}{x}\\right)\\frac{1}{|x|},\\qquad\n",
    "q(y,x) = \\frac12 f\\!\\left(\\frac{y}{x}\\right)\\frac{|y|}{x^{2}}.\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "\\frac{q(y,x)}{q(x,y)} = \\frac{|y|}{|x|}.\n",
    "$$\n",
    "\n",
    "### Case B: $|x|<|y|<1$\n",
    "\n",
    "Now\n",
    "$$\n",
    "q(x,y) = \\frac12 f\\!\\left(\\frac{x}{y}\\right)\\frac{|x|}{y^{2}},\\qquad\n",
    "q(y,x) = \\frac12 f\\!\\left(\\frac{x}{y}\\right)\\frac{1}{|y|},\n",
    "$$\n",
    "so again\n",
    "$$\n",
    "\\frac{q(y,x)}{q(x,y)} = \\frac{|y|}{|x|}.\n",
    "$$\n",
    "\n",
    "Thus in all cases\n",
    "$$\n",
    "\\frac{\\pi(y)\\,q(y,x)}{\\pi(x)\\,q(x,y)}\n",
    "=\n",
    "\\frac{\\pi(y)}{\\pi(x)}\\,\\frac{|y|}{|x|},\n",
    "$$\n",
    "and the Metropolis–Hastings acceptance probability is\n",
    "$$\n",
    "\\alpha(x,y)\n",
    "=\n",
    "1 \\wedge\n",
    "\\left(\n",
    "\\frac{\\pi(y)}{\\pi(x)}\\,\\frac{|y|}{|x|}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "With this choice of $\\alpha(x,y)$, the resulting Markov chain is\n",
    "reversible w.r.t. $\\pi$ (detailed balance), hence $\\pi$ is invariant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
